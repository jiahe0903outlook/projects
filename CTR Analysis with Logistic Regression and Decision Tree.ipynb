{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to extract insights from data\n",
    "\n",
    "\n",
    "The job of a data scientist is typically to extract insights from the data and, based on the insights, come up with ideas to improve the product.\n",
    "The standard approach is:\n",
    "\n",
    "1.\tCollect a dataset including your target variable (label) and variables that you think might be related\n",
    "\n",
    "2.\tBuild a model trying to predict the label\n",
    "\n",
    "3.\tLook into the model and figure out how each variable impacts the output\n",
    "\n",
    "4.\tBased on that, come up with product recommendation (aka the famous actionable insights you see in pretty much any DS job posting)\n",
    "\n",
    "Models and insights\n",
    "\n",
    "The most effective ways to extract insights from a model are:\n",
    "\n",
    "1.\tBuild a logistic or linear regression for, respectively, binary and continuous outputs, and look at the coefficients\n",
    "\n",
    "2.\tBuild a decision tree and look at its structure\n",
    "\n",
    "3.\tBuild any model and look at the model partial dependence plots\n",
    "\n",
    "4.\tBuild RuleFit and look at the dummy features it created\n",
    "\n",
    "Obviously, model insights are meaningful only if the model is predicting well. If a model predictive power is very bad, then looking at its structure is totally meaningless. However, checking and optimizing model performance is beyond the scope of this section.\n",
    "We’ll look now at each of those techniques in details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume you work in the marketing department and your product manager has asked you to get back to her with some project ideas on how to improve email click-through-rate. That is, the company has been sending marketing emails and they want to increase the percentage of people who click on the company link inside the email.\n",
    "You have a dataset like the one below. You can also download it from here.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "email_id\temail_text\temail_version\thour\tweekday\tuser_country\tuser_past_purchases\tclicked\n",
    "8\tshort_email\tgeneric\t9\tThursday\tUS\t3\t0\n",
    "33\tlong_email\tpersonalized\t6\tMonday\tUS\t0\t0\n",
    "46\tshort_email\tgeneric\t14\tTuesday\tUS\t3\t0\n",
    "49\tlong_email\tpersonalized\t11\tThursday\tUS\t10\t0\n",
    "65\tshort_email\tgeneric\t8\tWednesday\tUK\t3\t0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\temail_id : the Id of the email that was sent. It is unique by email\n",
    "*\temail_text : two different versions of the email have been sent: one has “long text” (i.e. has 4 paragraphs) and one has “short text” (just two paragraphs)\n",
    "*\temail_version : some emails were “personalized” (i.e. they had the name of the user receiving the email in the incipit, such as “Hi John,”), while some emails were “generic” (the incipit was just “Hi,”)\n",
    "*\thour : the local time on which the email was sent\n",
    "*\tweekday : the weekday on which the email was sent\n",
    "*\tuser_country : the country where the user receiving the email is based. It comes from the user ip address when they created the account\n",
    "*\tuser_past_purchases : how many items in the past were bought by the user receiving the email\n",
    "*\tclicked - Whether the user has clicked on the link inside the email. This is our label and, most importantly, the goal of the project is to increase this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions and Coefficients\n",
    "\n",
    "\n",
    "We will focus here on logistic regression given that the label we are trying to predict (“clicked”) is binary. However, the overall approach if you were dealing with a linear regression would be similar. After all, a logistic regression can be seen as a linear method with a particular link function (logit) to constrain the output between 0 and 1, so that it can be used for binary classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import statsmodels.api as sm\n",
    "pandas.set_option('display.max_columns', 10)\n",
    "pandas.set_option('display.width', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from google drive. This is the same dataset described in the previous section\n",
    "data = pandas.read_csv('https://drive.google.com/uc?export=download&id=1PXjbqSMu__d_ppEv92i_Gnx3kKgfvhFk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>email_text</th>\n",
       "      <th>email_version</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>user_country</th>\n",
       "      <th>user_past_purchases</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>short_email</td>\n",
       "      <td>generic</td>\n",
       "      <td>9</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>long_email</td>\n",
       "      <td>personalized</td>\n",
       "      <td>6</td>\n",
       "      <td>Monday</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>short_email</td>\n",
       "      <td>generic</td>\n",
       "      <td>14</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>long_email</td>\n",
       "      <td>personalized</td>\n",
       "      <td>11</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>US</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>short_email</td>\n",
       "      <td>generic</td>\n",
       "      <td>8</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>UK</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_id   email_text email_version  hour    weekday user_country  user_past_purchases  clicked\n",
       "0         8  short_email       generic     9   Thursday           US                    3        0\n",
       "1        33   long_email  personalized     6     Monday           US                    0        0\n",
       "2        46  short_email       generic    14    Tuesday           US                    3        0\n",
       "3        49   long_email  personalized    11   Thursday           US                   10        0\n",
       "4        65  short_email       generic     8  Wednesday           UK                    3        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email_text       long_email\n",
      "email_version       generic\n",
      "weekday              Friday\n",
      "user_country             ES\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Before building the regression, we need to know which ones are the reference levels for the categorical variables\n",
    "#only keep categorical variables\n",
    "data_categorical = data.select_dtypes(['object']).astype(\"category\") \n",
    "#find reference level, i.e. the first level\n",
    "print(data_categorical.apply(lambda x: x.cat.categories[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "email_text       long_email\n",
    "email_version       generic\n",
    "weekday              Friday\n",
    "user_country             ES\n",
    "dtype: object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dummy variables from categorical ones. Using one-hot encoding and drop_first=True \n",
    "data = pandas.get_dummies(data, drop_first=True)\n",
    "  \n",
    "#add intercept\n",
    "data['intercept'] = 1\n",
    "#drop the label\n",
    "train_cols = data.drop('clicked', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.092770\n",
      "         Iterations 9\n"
     ]
    }
   ],
   "source": [
    "#Build Logistic Regression\n",
    "logit = sm.Logit(data['clicked'], train_cols)\n",
    "output = logit.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Optimization terminated successfully.\n",
    "         Current function value: 0.092770\n",
    "         Iterations 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = pandas.DataFrame(dict(coefficients = output.params, SE = output.bse, z = output.tvalues, p_values = output.pvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            coefficients            SE          z       p_values\n",
      "email_id                   -3.848609e-08  7.780379e-08  -0.494656   6.208432e-01\n",
      "hour                        1.670684e-02  5.005879e-03   3.337445   8.455247e-04\n",
      "user_past_purchases         1.878107e-01  5.725787e-03  32.800855  5.725039e-236\n",
      "email_text_short_email      2.793085e-01  4.530477e-02   6.165101   7.043829e-10\n",
      "email_version_personalized  6.387251e-01  4.691461e-02  13.614631   3.277989e-42\n",
      "weekday_Monday              5.410326e-01  9.341014e-02   5.792011   6.954864e-09\n",
      "weekday_Saturday            2.828638e-01  9.777629e-02   2.892969   3.816190e-03\n",
      "weekday_Sunday              1.836278e-01  1.001194e-01   1.834088   6.664099e-02\n",
      "weekday_Thursday            6.254040e-01  9.233999e-02   6.772839   1.262790e-11\n",
      "weekday_Tuesday             6.162222e-01  9.237223e-02   6.671077   2.539336e-11\n",
      "weekday_Wednesday           7.554637e-01  9.084515e-02   8.315950   9.102053e-17\n",
      "user_country_FR            -7.864563e-02  1.625969e-01  -0.483685   6.286097e-01\n",
      "user_country_UK             1.155255e+00  1.220603e-01   9.464618   2.946372e-21\n",
      "user_country_US             1.141360e+00  1.159626e-01   9.842487   7.386228e-23\n",
      "intercept                  -6.880922e+00  1.560666e-01 -44.089646   0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "#get coefficients and pvalues\n",
    "print(output_table)\n",
    "                           "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "coefficients            SE          z       p_values\n",
    "email_id                   -3.848609e-08  7.780379e-08  -0.494656   6.208432e-01\n",
    "hour                        1.670684e-02  5.005879e-03   3.337445   8.455247e-04\n",
    "user_past_purchases         1.878107e-01  5.725787e-03  32.800855  5.725039e-236\n",
    "email_text_short_email      2.793085e-01  4.530477e-02   6.165101   7.043829e-10\n",
    "email_version_personalized  6.387251e-01  4.691461e-02  13.614631   3.277989e-42\n",
    "weekday_Monday              5.410326e-01  9.341014e-02   5.792011   6.954864e-09\n",
    "weekday_Saturday            2.828638e-01  9.777629e-02   2.892969   3.816190e-03\n",
    "weekday_Sunday              1.836278e-01  1.001194e-01   1.834088   6.664099e-02\n",
    "weekday_Thursday            6.254040e-01  9.233999e-02   6.772839   1.262790e-11\n",
    "weekday_Tuesday             6.162222e-01  9.237223e-02   6.671077   2.539336e-11\n",
    "weekday_Wednesday           7.554637e-01  9.084515e-02   8.315950   9.102053e-17\n",
    "user_country_FR            -7.864563e-02  1.625969e-01  -0.483685   6.286097e-01\n",
    "user_country_UK             1.155255e+00  1.220603e-01   9.464618   2.946372e-21\n",
    "user_country_US             1.141360e+00  1.159626e-01   9.842487   7.386228e-23\n",
    "intercept                  -6.880922e+00  1.560666e-01 -44.089646   0.000000e+00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            coefficients        SE          z       p_values\n",
      "user_country_UK                 1.155255  0.122060   9.464618   2.946372e-21\n",
      "user_country_US                 1.141360  0.115963   9.842487   7.386228e-23\n",
      "weekday_Wednesday               0.755464  0.090845   8.315950   9.102053e-17\n",
      "email_version_personalized      0.638725  0.046915  13.614631   3.277989e-42\n",
      "weekday_Thursday                0.625404  0.092340   6.772839   1.262790e-11\n",
      "weekday_Tuesday                 0.616222  0.092372   6.671077   2.539336e-11\n",
      "weekday_Monday                  0.541033  0.093410   5.792011   6.954864e-09\n",
      "weekday_Saturday                0.282864  0.097776   2.892969   3.816190e-03\n",
      "email_text_short_email          0.279308  0.045305   6.165101   7.043829e-10\n",
      "user_past_purchases             0.187811  0.005726  32.800855  5.725039e-236\n",
      "hour                            0.016707  0.005006   3.337445   8.455247e-04\n",
      "intercept                      -6.880922  0.156067 -44.089646   0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "#only keep significant variables and order results by coefficient value\n",
    "print(output_table.loc[output_table['p_values'] < 0.05].sort_values(\"coefficients\", ascending=False))\n",
    " \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                           coefficients        SE          z       p_values\n",
    "user_country_UK                 1.155255  0.122060   9.464618   2.946372e-21\n",
    "user_country_US                 1.141360  0.115963   9.842487   7.386228e-23\n",
    "weekday_Wednesday               0.755464  0.090845   8.315950   9.102053e-17\n",
    "email_version_personalized      0.638725  0.046915  13.614631   3.277989e-42\n",
    "weekday_Thursday                0.625404  0.092340   6.772839   1.262790e-11\n",
    "weekday_Tuesday                 0.616222  0.092372   6.671077   2.539336e-11\n",
    "weekday_Monday                  0.541033  0.093410   5.792011   6.954864e-09\n",
    "weekday_Saturday                0.282864  0.097776   2.892969   3.816190e-03\n",
    "email_text_short_email          0.279308  0.045305   6.165101   7.043829e-10\n",
    "user_past_purchases             0.187811  0.005726  32.800855  5.725039e-236\n",
    "hour                            0.016707  0.005006   3.337445   8.455247e-04\n",
    "intercept                      -6.880922  0.156067 -44.089646   0.000000e+00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一些continuous variable, 首先不会significant，otherwise misleading. 原本coefficient>0说明越大越好，To solve this, you should manually create segments (i.e. indicator variables) before building the model. One segment could be night time, one morning to noon, etc.\n",
    "\n",
    "○\tMore importantly, note the super low coefficient for email_id compared to the other ones. That doesn’t mean that the variable is irrelevant. The super low coefficient simply depends on the fact that email_id scale is way larger than the other variables. The max value of all other variables is 24 for hour. The max value of email_id is 100K! So the low coefficient is meant to balance the different scale, otherwise email_id would entirely drive the regression output.\n",
    "\n",
    "Only thing, looking at the scale of the intercept vs the scale of the other coefficients * the possible values of those variables can be useful to get a sense of by how much you can affect the output\n",
    "\n",
    "■\t-> If I send emails on Wednesday, that variable value becomes 0.7 (i.e. 0.7 coefficient times the value of the variable that would be 1) which is pretty high relative to the -6.8 intercept. So opportunities of meaningful improvements are there. Imagine my intercept were -1000 and Wednesday coefficient were the same. Then optimizing the day would be almost irrelevant from a practical standpoint.\n",
    "\n",
    "✓ The absolute value of a coefficient is often used to quickly estimate variable importance. However, that depends on the variable scale more than anything else. You could normalize variables, so they are all on the same scale. But that’s rarely a good idea if your goal is presenting to product people. It is hard to get a product manager excited by saying: “If we increase variable X by one standard deviation, we could achieve this and that”\"\n",
    "delta z = 1, 等价于x - x0 = sigma(sd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Understanding the output\n",
    "\n",
    "●\tCategorical Variables\n",
    "\n",
    "\n",
    "○\tAll categorical variables are encoded via one-hot encoding. If there are n levels within a categorical variable, we are creating n-1 dummy variables. The remaining level is the reference level or baseline\n",
    "\n",
    "○\tFor instance, weekday has 6 levels in the regression: Monday, Saturday, Sunday, Thursday, Tuesday, and Wednesday. The missing one Friday is the baseline\n",
    "\n",
    "○\tThe way to interpret the outcome for categorical variables is that the coefficient of those levels is relative to the missing level. All days are better than Friday, although Sunday is not statistically significantly better\n",
    "\n",
    "○\tIf you see all negative (positive) and significant coefficients for a given categorical variable doesn’t mean that they are all bad (good) in absolute terms. It simply means that they are all worse (better) than the reference level\n",
    "\n",
    "○\tThere are quite a few cases in which you want to specifically set the reference level. For instance, when you have one level which is by far the most common and you want to compare all other levels against that. This is especially common if you are looking for growth opportunities. Let’s take country as an example, you might want your most important country as reference level. Or if you are looking into new marketing channels to see which one is the most promising one, it would be beneficial to compare them against your current best one. The resulting levels with positive and significant coefficients would be a goldmine of information from a growth standpoint\n",
    "\n",
    "\n",
    "\n",
    "●\tGeneral Insights\n",
    "\n",
    "\n",
    "○\tUser country seems very important. Especially interesting is that English speaking countries (US, UK) are doing significantly better than non-English speaking countries (ES, FR). That could mean a bad translation or in general a non-localized version of the email. The first thing you want to do here is probably getting in touch with the international team and asking them to review French and Spanish email templates\n",
    "\n",
    "○\tNot surprisingly, all weekday coefficients are positive. Sunday is (barely) non-significant, all others are significant. This is a consequence of having Friday as reference level. It is a well-known fact that sending marketing emails on Friday is not a great idea. Wednesday seems to be the best day, but in general all week days (Monday-Thursday) perform similarly. Friday - Sunday are much worse. The company should probably start sending emails only Monday-Thursday, with a particular focus on the middle of the week\n",
    "\n",
    "○\tPersonalized emails are doing better. So the company should stop sending generic emails. But most importantly, this can be a huge insight from a product standpoint. If just adding the name at the top is increasing clicks significantly, imagine what would happen with even more personalization. Definitely worth investing in this\n",
    "\n",
    "○\tSending short emails appears to be better, but personalizing emails should be the priority vs finding a general optimal email template that on an average works best for everyone (see much lower coefficient compared to the personalization one)\n",
    "\n",
    "○\tHour perfectly emphasizes the problems of logistic regressions with numerical variables. The best time is likely during the day and early mornings and late nights are probably bad. But the model is trying to find a linear relationship between hour and the output. In most cases, this means that will not find a significant relationship. If it does find significance, the results would be highly misleading. Like in this case, it is telling us that the larger the value of hour, the better it is. So the best time would be 24 (midnight)! To solve this, you should manually create segments (i.e. indicator variables) before building the model. One segment could be night time, one morning to noon, etc.\n",
    "\n",
    "○\tEmail_id is not significant, but the p-value is not that high either, so it is something to keep in mind. Email_id could be interesting because it can be seen as a proxy for time, i.e. the first email sent gets id 1, second id 2, etc. So a significant and negative coefficient would mean that as time goes by, less and less people are clicking on the email. This could be a big red flag, like for instance Google started labeling us as spam. It doesn’t look like the case here, but still, it is something to keep in mind\n",
    "\n",
    "○\tMore importantly, note the super low coefficient for email_id compared to the other ones. That doesn’t mean that the variable is irrelevant. The super low coefficient simply depends on the fact that email_id scale is way larger than the other variables. The max value of all other variables is 24 for hour. The max value of email_id is 100K! So the low coefficient is meant to balance the different scale, otherwise email_id would entirely drive the regression output.\n",
    "\n",
    "○\tThe intercept highly negative and significant is the regression outcome if all variables are set to zero. So, basically, categorical variables are all set to their reference levels and numerical variables are set to 0. Intercepts are almost always negative and significant given that in the majority of cases you are dealing with imbalanced classes, where 1s are <5% of the events. And in a logistic regression a negative outcome means higher probability of predicting class zero. Don’t read too much into it. After all, the all-values-are-0 scenario is unrealistic at best, and often impossible. Like here “hour” is coded as from 1 to 24, so it cannot even have the value 0! Only thing, looking at the scale of the intercept vs the scale of the other coefficients * the possible values of those variables can be useful to get a sense of by how much you can affect the output\n",
    "\n",
    "■\t-> If I send emails on Wednesday, that variable value becomes 0.7 (i.e. 0.7 coefficient times the value of the variable that would be 1) which is pretty high relative to the -6.8 intercept. So opportunities of meaningful improvements are there. Imagine my intercept were -1000 and Wednesday coefficient were the same. Then optimizing the day would be almost irrelevant from a practical standpoint.\n",
    "\n",
    "\n",
    "Pros and Cons\n",
    "Pros of using logistic regression coefficients to extract insights from data\n",
    "\n",
    "\n",
    "✓ Pretty much anyone in a technical or product management role in a tech company is familiar with logistic regressions (if this is not true at your company, you are probably working in the wrong place). It is so much easier to present data science work if the audience is already familiar with the techniques used\n",
    "\n",
    "✓ Logistic regressions are by far the most used model in production. Despite all the blog posts, conference talks, etc. about deep learning, it is almost guaranteed that a consumer tech company most important model in production will be a logistic regression. Therefore, it will be easy to collaborate with engineers (i.e. leveraging prior work done by them, helping them improve their model, etc.)\n",
    "\n",
    "✓ It is simple, fast, and generally reliable. Indeed, building the model is straightforward. The model works well in the majority of cases and all you have to do is look at the coefficient values and their p-values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cons of using logistic regression coefficients to extract insights from data\n",
    "\n",
    "\n",
    "✓ Coefficients give an idea of the impact of each variable on the output. But it is actually pretty hard to exactly visualize what that means. I.e., a change in a given variable by one unit changes the log odds ratio by \n",
    "β\n",
    "β units, where \n",
    "β\n",
    "β is the coefficient. Mmh…\n",
    "\n",
    "✓ Coefficients do not allow to segment a variable. For instance, a positive coefficient in front of variable age means that as age increases, the output keeps increasing as well. Always. This is unlikely to be true for most numerical variables. You often need to create segments before building the regression (btw RuleFit solves exactly this problem)\n",
    "\n",
    "✓ Coefficient meaning in front of a categorical variable with several levels can be confusing. You change a given variable reference level and all other level coefficients change\n",
    "\n",
    "✓ The absolute value of a coefficient is often used to quickly estimate variable importance. However, that depends on the variable scale more than anything else. You could normalize variables, so they are all on the same scale. But that’s rarely a good idea if your goal is presenting to product people. It is hard to get a product manager excited by saying: “If we increase variable X by one standard deviation, we could achieve this and that”\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "\n",
    "Unlike regressions, Decision Trees (DT) are very good when the relationship between variables and outcome is non-linear. The best possible example is the behavior of the variable “hour” in the email click data set. While regressions would fail in finding the non-linearity, trees would easily identify that probability of clicks is high in the morning/early afternoon and low outside this segment. Also, trees are really good in looking at how variables interact with each other, by automatically creating segments including multiple variables. We will see all this in details.\n",
    "\n",
    "Perhaps even more importantly, trees can be very useful in practice when it comes to building metrics, which is one of the most important tasks of a data scientist. The high high majority of metrics are based on the idea of finding one hard threshold that separates “good” from “bad” and then trying to increase the percentage of users falling in the good bucket. For instance:\n",
    "\n",
    "●\tEarly FB growth metric: users with at least X friends in Y days\n",
    "\n",
    "●\tEngagement: users performing at least X actions per day\n",
    "\n",
    "●\tResponse rate: proportion of questions with at least 1 answer within X hour\n",
    "\n",
    "●\tConversion rate: proportion of users who convert within X time since their first visit\n",
    "\n",
    "And so on. It is hard to think about one single metric that doesn’t use the template above. And to find those X and Y thresholds in the metrics above, trees can be very useful. Like going back to the email click dataset, we know by now that the higher the number of purchases, the more engaged is the customer (fairly obviously). But is there a threshold based on which we can define customers as “power users” vs “non-power user”? If so, we could then create a metric like:\n",
    "\n",
    "●\tpower user: user with > X purchases\n",
    "\n",
    "And then build a team whose goal is to increase the percentage of power users within the customer user base. There is hardly something more effective than this approach. And really the famous FB growth metric 7 friends in 10 days has exactly this idea.\n",
    "\n",
    "Let’s now build the tree and see in practice how to use its output for insights using the same email dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Decision Trees\n",
    "\n",
    "●\tR\n",
    "●\tPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading https://files.pythonhosted.org/packages/83/cc/c62100906d30f95d46451c15eb407da7db201e30f42008f3643945910373/graphviz-0.14-py2.py3-none-any.whl\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.001, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas \n",
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "  \n",
    "#Read from google drive. Always the same dataset.\n",
    "data = pandas.read_csv('https://drive.google.com/uc?export=download&id=1PXjbqSMu__d_ppEv92i_Gnx3kKgfvhFk')\n",
    "  \n",
    "#prepare the data for the model by creating dummy vars and removing the label\n",
    "data_dummy = pandas.get_dummies(data, drop_first=True)\n",
    "train_cols = data_dummy.drop('clicked', axis=1)\n",
    "  \n",
    "#build the tree\n",
    "tree=DecisionTreeClassifier(\n",
    "    #set max tree dept at 4. Bigger than that it just becomes too messy\n",
    "    max_depth=4,\n",
    "    #change weights given that we have unbalanced classes. Our data set is now perfectly balanced. It makes easier to look at tree output\n",
    "    class_weight=\"balanced\",\n",
    "    #only split if it's worthwhile. The default value of 0 means always split no matter what if you can increase overall performance, which might lead to irrelevant splits\n",
    "    min_impurity_decrease = 0.001\n",
    "    )\n",
    "tree.fit(train_cols,data_dummy['clicked'])\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Tpdf', '-O', 'tree.dot'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mD:\\Users\\46487\\Anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\46487\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\46487\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-689cb1ccce00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdot_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tree.dot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\46487\\Anaconda3\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36mview\u001b[1;34m(self, filename, directory, cleanup, quiet, quiet_view)\u001b[0m\n\u001b[0;32m    240\u001b[0m         return self.render(filename=filename, directory=directory,\n\u001b[0;32m    241\u001b[0m                            \u001b[0mview\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                            quiet=quiet, quiet_view=quiet_view)\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\46487\\Anaconda3\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[0;32m    207\u001b[0m         rendered = backend.render(self._engine, format, filepath,\n\u001b[0;32m    208\u001b[0m                                   \u001b[0mrenderer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                                   quiet=quiet)\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\46487\\Anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\46487\\Anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Tpdf', '-O', 'tree.dot'], make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "#visualize it\n",
    "export_graphviz(tree, out_file=\"tree.dot\", feature_names=train_cols.columns, proportion=True, rotate=True)\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "s = Source.from_file(\"tree.dot\")\n",
    "s.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以对于同一个feature连续split两次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to interpret the output is:\n",
    "\n",
    "●\tEach block is a tree node. The nodes all the way to the right are called leaves and the final model classification depends on the leaf where an event ends up\n",
    "\n",
    "●\tWithin a node you have 4 values:\n",
    "1.\tThe split. This is the split that leads to the two nodes to the right\n",
    "2.\tThe gini index of that node. It represents purity of the node. 0.5 means random guess, so it is the worst possible value. 0 means perfect classification, so it is the best possible value. Look for nodes with values as close as possible to 0\n",
    "3.\tSamples: the proportion of events in that node. The higher the better. It means that node is very important cause captures many people\n",
    "4.\tValue: proportion of class 0 and class 1 events. The sum of those two values is always 1. Similarly to Gini, it gives an idea of how pure the node is. Ideally, you want one of the two values to be close to 1 and the other to be close to 0. That’s when Gini will be small. If the first of those two values is higher than 0.5, the node is labeled as class 0. Otherwise it is class 1 node\n",
    "\n",
    "●\tSo our starting point is the first node to the left. There we have 100% of samples (obviously, we haven’t even started splitting), the proportion between classes is a perfect 50/50 (we balanced the data before building the model), and, therefore, the gini is 0.5, as bad as it can possibly be. From this node, the first split is on user_past_purchases <= 3.5. The way you read that is:\n",
    "1.\tIf a user has <= 3.5 purchases, follow the True arrow. So you end up in the node up and right. That node has Gini of 0.44 (so we improved), 52.7% events (meaning 52.7% of people in our dataset have <= 3.5 purchases), and of those 52.7% of people 67.4% did not click while 32.6% did click. So this is a 0 class node. That split helped us identify a segment with a lower proportion of clicks compared to the starting point of 50/50.\n",
    "2.\tIf you go right and down, you are following the False arrow. So you end up in a node representing people with > 3.5 purchases (i.e. not true that they have <= 3.5 purchases). In this case Gini is 0.474, samples is 47.3% and class 0/class 1 proportions are, respectively, 38.6% and 61.4%. So here we found a segment with a significantly higher percentage of people who click\n",
    "\n",
    "●\tLet’s now move one more step to the right. Let’s consider the node up/right, the one with 0.44, 52.7% and .674/.326 values inside. This is the starting point for the new split, which is user_past_purchases <= 0.5.\n",
    "1.\tAs before, up means true, down means false. So if we go up, we find users who have <= 3.5 purchases AND <= 0.5 purchases. Since I am splitting on the same variable twice, this is the same as simply saying <= 0.5 purchases. We only have 13.9% of total users there and only 0.7% of them click! This is a really interesting node because it is so pure. Almost achieves perfect classification, as you can see from the super low gini.\n",
    "2.\tIf we go down, we find users who have <= 3.5 purchases AND > 0.5 purchases. Basically, between 1 and 3 purchases. We have 38.9% of total users there and almost 40% of them click. The percentage of users who click is higher than the previous node. This means that by removing users with 0 purchases, we managed to find a better segment for our label\n",
    "\n",
    "●\tAs you keep going right this way, you get to the leaves, which are the final classification of the tree. For instance, let’s take the leaf all the way up/right. Those are users with:\n",
    "1.\t<= 3.5 purchases AND\n",
    "2.\tMore than 0.5 purchases (i.e. false that they have <= 0.5 purchases) AND\n",
    "3.\tEmail_version_personalized <= 0.5 (meaning email is not personalized) AND\n",
    "4.\tUser_country_France <= 0.5 (meaning the user is not from France)\n",
    "These users represent 17.5% of total users. Out of those users, 68.9% don’t click and the remaining 31.1% click. So this is a class 0 leaf. If an event ends up there, we predict that will not click.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Insights\n",
    "\n",
    "●\tBy far the most important insight from a tree is given by the first split. This model is telling us that the most important segment is whether users bought more or less than 3 times in the past. Increasing the proportion of users with more than 3 purchases would be a great company-wide yearly goal.\n",
    "We don’t have timestamp of the purchases here, but if we had that we could see if the tree also splits on that and create a metric like: percentage of users with at least 3 purchases within X time\n",
    "\n",
    "●\tIf a user has zero purchases, the tree doesn’t split on any other variable. That’s a leaf. This means that everything else becomes irrelevant if a user has never bought. Changing time of the day, weekday, subject, etc, makes no difference there. To make these users click the change will need to be much more dramatic than just changing the email template or when to send it.\n",
    "The next step should be crafting a totally different email with a different message just for these users. These are the hard users to win, but they are also where more value is. They already came to the site and gave their email address, so they have some sort of intent. But they never bought anything. Understanding why that happened could unlock so much value and is way easier to get these people to buy vs having to get new users and then trying to make them convert\n",
    "\n",
    "●\tCountry UK/US and email_personalized = TRUE always lead to higher proportion of clicks, in both R and Python. R also splits on weekday with a clear weekend/weekday split, i.e. one side is Friday/Saturday/Sunday and the other side is the other days. Note that R can split on multiple levels at the same time, while Python will look at each dummy variable independently. In practical terms, this means that Python will need larger trees to extract that information. I.e. to split on those 3 days would require to go down 3 times, firstly splitting on say Friday, then Saturday, and then Sunday.\n",
    "Also, a split on multiple levels has more power than a split on just one level, i.e. can separate the classes better. So always expect categorical variables with many levels to look more important in R than Python\n",
    "\n",
    "●\tThere is no split on any other variables beside those above. This depends on the fact that we built rather small trees, so the tree only focused on macro-information. Beside the fact that is really hard to visualize large trees, splits in large trees are not that informative either.\n",
    "If I had a split at the bottom on the variable hour, this would be conditional on all the previous splits, like purchases < X AND email_personalized = Y AND purchases > Z AND country = J, etc. So it wouldn’t tell me in absolute terms when it is the best time to send an email, but only for that specific segment. And given how specific that segment would be, there would be few events in that node, so overall it would not be particularly important\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
